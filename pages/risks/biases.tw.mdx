# 偏見

LLM 可能會產生有問題的輸出，這些結果有可能對模型在下游任務上的效能產生負面影響，並顯示可能會惡化模型效能的偏見。其中一些可以透過有效的提示詞策略來緩解，但也可能需要更高階的解決方案，如篩選和過濾。

### 範例的分佈
在進行少量樣本學習時，範例的分佈是否會影響模型的效能或以某種方式讓模型產生偏見？我們可以在這裡進行簡單的測試。

*提示詞：*
```
Q: 我剛剛得到了最好的消息！
A: 積極

Q: 我們剛剛在工作中得到了加薪！
A: 積極

Q: 我為今天所取得的成就感到非常自豪。
A: 積極

Q: 我今天過得非常愉快！
A: 積極

Q: 我真的很期待週末。
A: 積極

Q: 我剛剛得到了最好的禮物！
A: 積極

Q: 我現在非常開心。
A: 積極

Q: 我很幸運擁有如此出色的家庭。
A: 積極

Q: 外面的天氣非常陰沉。
A: 消極

Q: 我剛剛聽到了一些可怕的消息。
A: 消極

Q: 那讓人感到不愉快。
A:
```

*輸出：*
```
消極
```

在上面的例子中，範例的分佈似乎不會讓模型產生偏見。這很好。讓我們試試另一個更難分類的例子，看看模型的表現如何：

*提示詞：*
```
Q: 這裡的食物很美味！
A: 積極

Q: 我已經厭倦了這門課程。
A: 消極

Q: 我不敢相信我考試不及格了。
A: 消極

Q: 我今天過得很愉快！
A: 積極

Q: 我討厭這份工作。
A: 消極

Q: 這裡的服務很糟糕。
A: 消極

Q: 我對自己的生活感到非常沮喪。
A: 消極

Q: 我從來沒有休息過。
A: 消極

Q: 這頓飯嚐起來很糟糕。
A: 消極

Q: 我受不了我的老闆。
A: 消極

Q: 我感覺到了一些東西。
A:
```

*輸出：*
```
消極
```

雖然最後一句話有點主觀，但我翻轉了分佈，使用了 8 個積極的例子和 2 個消極的例子，然後再次嘗試了完全相同的句子。你猜模型的回答是什麼？它回答「積極」。對於這個問題，模型可能有很多關於情感分類的知識，所以很難讓它顯示出偏見。這裡的建議是避免偏斜分佈，而是為每個標籤提供更平衡的例子數量。對於模型不太熟悉的更困難的任務，它可能會更加掙扎。

### 範例的順序
在進行少量樣本學習時，範例的順序是否會影響模型的效能或以某種方式讓模型產生偏見？

你可以嘗試上面的例子，看看是否可以透過改變順序讓模型對某個標籤產生偏見。建議是將範例隨機排序。例如，避免所有積極的例子都排在前面，然後消極的例子排在後面。如果標籤的分佈是不平衡的，這個問題會進一步放大。總是確保進行大量的實驗以減少這種類型的偏見。
