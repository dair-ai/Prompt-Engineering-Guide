# LLM 설정

프롬프트를 사용할 때, API를 사용하거나 직접 대규모언어모델(LLM)과 상호 작용 할 수 있습니다. 몇 가지 파라미터를 설정하여 프롬프트에서 여러 가지 결과를 얻을 수 있습니다.

**temperature** - 요컨대, `temperature` 값이 낮을수록 항상 가장 확률이 높은 토큰(말뭉치의 최소 단위)이 선택되기 때문에 더 결정론적인 결과를 낳습니다. temperature 값을 높였을 때 모델이 선택하는 토큰의 무작위성이 증가하여 보다 다양하고 창조적인 결과를 촉진합니다. 이는 다른 가능한 토큰의 가중치를 증가시키는 것과 같습니다. 애플리케이션의 경우, 사실을 기반으로 하는 질의응답과 같은 작업에는 낮은 temperature 값을 사용하여 보다 사실적이고 간결한 응답을 얻을 수 있습니다. 시를 생성하는 등 다른 창의적인 작업의 경우에는 temperature 값을 높이는 것이 도움이 될 수 있습니다.

**top_p** - 마찬가지로, temperature를 활용하는 핵 샘플링 기법인 `top_p`를 사용하면 모델이 응답을 생성하는 결정성을 제어할 수 있습니다. 정확하고 사실적인 답변을 원한다면 이를 낮게 유지합니다. 더 다양한 반응을 원한다면 더 높은 값으로 증가시킵니다.

일반적인 권장 사항은 둘 중 하나만 변경하는 것입니다.

**Max Length** - `max length`를 조정하여 모델이 생성하는 토큰의 수를 관리할 수 있습니다. max length를 지정하면 길거나 관련 없는 응답을 방지하고 제어 비용을 관리하는데 도움이 될 수 있습니다.

**Stop Sequences** - `stop sequence`는 모델의 토큰 생성을 중지하는 문자열입니다. stop sequences를 지정하는 것은 모델의 응답 길이 및 구조를 제어하는데 도움이 될 수 있습니다. 예를 들어, stop sequence로 "11"을 추가하여 항목이 10개를 초과하지 않는 리스트를 생성하도록 모델에 지시할 수 있습니다.

**Frequency Penalty** - `frequency penalty`는 해당 토큰이 응답 및 프롬프트에 등장한 빈도에 비례하여 다음에 등장할 토큰에 불이익을 적용합니다. frequency penalty가 높을수록 단어가 다시 등장할 가능성이 줄어듭니다. 이 설정은 자주 등장하는 토큰에 대하여 더 많은 페널티를 부여하여 모델의 응답에서 단어의 반복을 방지합니다.

**Presence Penalty** - `presence penalty`는 반복되는 토큰에 패널티를 적용하지만, frequency penalty와 달리 모든 토큰에 동일한 페널티가 적용됩니다. 다시 말해, 토큰이 2회 등장하는 토큰과 10회 등장하는 토큰이 동일한 페널티를 받습니다. 이 설정은 모델이 응답에서 구문을 너무 자주 반복하는 것을 방지합니다. 다양하거나 창의적인 텍스트를 생성하기 위해 더 높은 presence penalty를 사용할 수 있습니다. 혹은 모델이 집중력을 유지해야 할 경우(사실을 기반으로) 더 낮은 presence penalty를 사용할 수 있습니다.

`temperature` 및 `top_p`와 유사하게, 일반적인 권장 사항은 frequency penalty와 presence penalty 중 하나만 변경하거나 둘 다 변경하지 않는 것입니다.

몇 가지 기본적인 예시를 살펴보기에 앞서, 사용하는 LLM 버전에 따라 결과가 상이할 수 있음을 알립니다.
