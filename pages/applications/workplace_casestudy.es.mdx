# Caso de estudio de clasificación de trabajo de graduados

[Clavié et al., 2023](https://arxiv.org/abs/2303.07142) proporcionan un caso de estudio sobre la ingeniería de prompts aplicada a un caso de uso de clasificación de texto a mediana escala en un sistema de producción. Utilizando la tarea de clasificar si un trabajo es un verdadero "trabajo básico", adecuado para un recién graduado, o no, evaluaron una serie de técnicas de ingeniería de prompts y reportaron sus resultados utilizando GPT-3.5 (`gpt-3.5-turbo`).

El trabajo muestra que los LLM superan a todos los demás modelos probados, incluyendo una base de referencia extremadamente fuerte en DeBERTa-V3. `gpt-3.5-turbo` también supera notablemente a las variantes más antiguas de GPT3 en todas las métricas clave, pero requiere un análisis adicional de la salida ya que su capacidad para mantenerse en una plantilla parece ser peor que las otras variantes.

Los hallazgos clave de su enfoque de ingeniería de prompts son:

- Para tareas como esta, donde no se requiere conocimiento experto, CoT con pocos ejemplos dio peores resultados que la generación sin ejemplos en todos los experimentos.
- El impacto del prompt en la obtención del razonamiento correcto es enorme. Simplemente pedir al modelo que clasifique un trabajo dado da como resultado una puntuación F1 de 65.6, mientras que el modelo de ingeniería posterior al prompt logra una puntuación F1 de 91.7.
- Intentar forzar al modelo a mantenerse en una plantilla disminuye el rendimiento en todos los casos (este comportamiento desaparece en las primeras pruebas con GPT-4, que son posteriores al documento).
- Muchas modificaciones pequeñas tienen un gran impacto en el rendimiento.
  - Las tablas a continuación muestran las modificaciones completas probadas.
  - Dar instrucciones adecuadas y repetir los puntos clave parece ser el factor más importante para el rendimiento.
  - Algo tan simple como dar al modelo un nombre (humano) y referirse a él como tal aumentó la puntuación F1 en 0.6 puntos.

### Modificaciones the prompt probadas

| Nombre corto | Descripción                                                                                         |
|--------------|-----------------------------------------------------------------------------------------------------|
| Baseline     | Proporciona una descripción del trabajo y pregunta si es adecuada para un graduado.                 |
| CoT          | Da algunos ejemplos de clasificación precisa antes de realizar la consulta.                         |
| Zero-CoT     | Pide al modelo que razone paso a paso antes de proporcionar su respuesta.                           |
| rawinst      | Da instrucciones sobre su papel y la tarea agregando al mensaje del usuario.                        |
| sysinst      | Da instrucciones sobre su papel y la tarea como mensaje del sistema.                                |
| bothinst     | Divide las instrucciones con el papel como mensaje del sistema y la tarea como mensaje del usuario. |
| mock         | Da instrucciones sobre la tarea burlándose de una discusión donde las reconoce.                     |
| reit         | Refuerza los elementos clave en las instrucciones repitiéndolos.                                    |
| strict       | Pide al modelo que responda siguiendo estrictamente una plantilla dada.                             |
| loose        | Pide que solo se proporcione la respuesta final siguiendo una plantilla dada.                       |
| right        | Pide al modelo que llegue a la conclusión correcta.                                                 |
| info         | Proporciona información adicional para abordar fallas de razonamiento comunes.                      |
| name         | Da al modelo un nombre con el que nos referimos a él en la conversación.                            |
| pos          | Proporciona retroalimentación positiva al modelo antes de hacer la consulta.                        |



### Impacto de rendimiento de todas las modificaciones de prompt

|                                        | Precision     | Recall        | F1            | Apego a la Plantilla   |
|----------------------------------------|---------------|---------------|---------------|------------------------|
| _Baseline_                             | _61.2_        | _70.6_        | _65.6_        | _79%_                  |
| _CoT_                                  | _72.6_        | _85.1_        | _78.4_        | _87%_                  |
| _Zero-CoT_                             | _75.5_        | _88.3_        | _81.4_        | _65%_                  |
| _+rawinst_                             | _80_          | _92.4_        | _85.8_        | _68%_                  |
| _+sysinst_                             | _77.7_        | _90.9_        | _83.8_        | _69%_                  |
| _+bothinst_                            | _81.9_        | _93.9_        | _87.5_        | _71%_                  |
| +bothinst+mock                         | 83.3          | 95.1          | 88.8          | 74%                    |
| +bothinst+mock+reit                    | 83.8          | 95.5          | 89.3          | 75%                    |
| _+bothinst+mock+reit+strict_           | _79.9_        | _93.7_        | _86.3_        | _**98%**_              |
| _+bothinst+mock+reit+loose_            | _80.5_        | _94.8_        | _87.1_        | _95%_                  |
| +bothinst+mock+reit+right              | 84            | 95.9          | 89.6          | 77%                    |
| +bothinst+mock+reit+right+info         | 84.9          | 96.5          | 90.3          | 77%                    |
| +bothinst+mock+reit+right+info+name    | 85.7          | 96.8          | 90.9          | 79%                    |
| +bothinst+mock+reit+right+info+name+pos| **86.9**      | **97**        | **91.7**      | 81%                    |

El apego a la plantilla se refiere a qué tan frecuentemente el modelo responde en el formato deseado.
