# 생성 데이터셋 다양성 다루기

import {Screenshot} from 'components/screenshot'

import IMG1 from '../../img/synthetic_diversity/textbooks_1.png'
import IMG2 from '../../img/synthetic_diversity/textbooks_2.png'

이전 [챕터](https://www.promptingguide.ai/applications/synthetic_rag)에서, 합성 데이터셋 생성에 LLM을 사용하여 로컬 Retriever 모델을 추가 파인튜닝하는 잠재력에 대해 다뤄보았습니다. 이런 방법은 라벨링 되지않은 문서들의 대량의 말뭉치(corpus)가 있었기에 가능했습니다. 각 문서들은 하나 이상의 합성 쿼리와 쿼리-문서(query-document) 쌍을 생성하는데 사용됩니다.

하지만 정보 검색(Information Retrieval)이 당신의 업무가 아니라면? 당신은 법적 문서 분류를 처리해야하고 그 어느 데이터도 외부 API로 내보낼 수 없는 상황에 놓여있다고 가정해보겠습니다. 이러한 상황에서는 로컬 모델을 학습시켜야 하겠지요. 하지만, 데이터 수집은 엄청난 장벽이고 프로덕트 개발에 병목을 일으키죠.

더 단순한 예시로, 어린이를 위한 이야기 만들기를 목표로 삼아보겠습니다. 이것은 [Eldan et al. (2023)](https://arxiv.org/abs/2305.07759)가 실시한 연구의 출발점이었습니다. 각 이야기는 2-3개의 문단의 단도직입적인 줄거리와 주제를 다루는 동시에 전체 데이터셋은 아이들이 이해할 수 있는 수준의 단어와 사실 기반의 지식을 다뤄야합니다.

언어는 단순히 규칙과 상징의 체계가 아닌 의미를 전달하고 풀이하는 것입니다. 대규모 언어 모델을 사용하여 훈련 데이터를 생성하는 주요 과제는 데이터셋의 다양성을 보장하는 것입니다. 높은 [generation temperature](https://www.promptingguide.ai/introduction/settings)에도 불구, 모델은 다양성이 부족한 반복적인 데이터셋을 생성할 수 있습니다 (심지어 어린이 수준의 언어로 쓰였는데도요). 일관성(coherence)과 관련성(relevance)은 자연어 생성이 직면한 또다른 과제입니다.

이러한 다양성 문제를 해결하기 위해 저자들은 전형적인 어린이가 쓸법한 어휘를 명사, 동사, 형용사로 구분한 뒤 1500개 내외의 기본 단어의 반영을 준비했습니다. 각 생성물(generation)에서 하나의 동사, 하나의 명사, 하나의 형용사를 무작위 선택한 뒤 모델은 이러한 무작위 단어를 조합하여 이야기를 생성합니다.

이 기법은 데이터셋의 다양성을 넓게 확장하여 이야기가 어린이의 전체 어휘와 다양한 개념 조합을 포괄하도록 분명히 하는 작업입니다. 또한 저자들은 대화, 줄거리 반전, 나쁜 결말, 또는 도덕적 교훈 같은 이야기의 특성을 통합했습니다. 각 이야기에 대해 이러한 특징의 무작위 부분 집합을 선택하고 모델에 이 특성을 포함하도록 프롬프트 작업을 했습니다.

*프롬프트:*
```
3살 어린이가 이해할 수 있을 정도의 아주 간단한 단어와 3-5 문단으로 이루어진 짧은 이야기를 지어내줘. 이 이야기는 다음과 같은 동사 ”{random.choice(동사_리스트)}”를, 명사는 ”{random.choice(명사_리스트)}” 그리고 형용사는 ”{random.choice(형용사_리스트)}”를 포함하도록 해줘. 그리고 이야기는 다음과 같은 특성을 포함해줘: {random.choice(특성_리스트)}, {random.choice(특성_리스트)}. 단순한 단어만 사용하는 것 잊지말아줘!
```

아래와 같이 공백(예: ”{random.choice(동사_리스트)}”)이 채워졌다고 가정해 보겠습니다:

*프롬프트:*
```
3살 어린이가 이해할 수 있을 정도의 아주 간단한 단어와 3-5 문단으로 이루어진 짧은 이야기를 지어내줘. 이 이야기는 다음과 같은 동사 ”꾸미다”를, 명사는 ”천둥” 그리고 형용사는 ”오래된”. 그리고 이야기는 다음과 같은 특성을 포함해줘: 이야기는 최소 하나 이상의 대화를 포함한다, 이야기는 나쁜 결말을 맞이한다. 단순한 단어만 사용하는 것 잊지말아줘!
```

*결과물:*
```
옛날 옛적에, 오래된 집에서, Lily라는 소녀가 살았어요. 그녀는 예쁜 물건으로 집 꾸미는 것을 좋아했어요. 어느 날, 그녀는 다락에서 큰 상자를 발견했어요. 상자를 열자 반짝이는 장식물이 나왔어요. Lily는 매우 행복했고 그것들로 집을 꾸미기로 했어요. Lily가 방을 꾸미고 있을 때, 바깥 하늘이 어두워졌어요. 큰 천둥이 울리고 Lily는 겁에 질렸어요. 그녀는 엄마에게 달려가 말했어요. ”엄마, 천둥 소리가 너무 커요!” 엄마는 그녀를 안아주면서 ”걱정마, 곧 지나갈거야.” 라고 했어요. 하지만 천둥은 멈추질 않고 점점 커져만갔고 오래된 집은 흔들리기 시작했어요. 그러더니 갑자기,예쁜 장식들 위로 지붕이 무너졌어요. Lily는 망가진 방을 보고 속상했답니다. 끝.
```

방대한 양의 합성 콘텐츠를 생성할 수 있는 최신 텍스트 생성 모델(GPT-3.5과 GPT-4)에 기반 하여 지시를 내려보았습니다. 매번 무작위로 조직화 한 프롬프트를 생성하고 모델은 이를 정확하게 지키기 때문에 매우 다양한 이야기가 생성됩니다. 한 두 단어가 같더라도 이야기는 완전히 다르게 전개될 것입니다. 이 같은 기법을 적용하여 프롬프트에 무작위성을 주입(inject)하여 다양한 데이터셋을 생성합니다.

여기서 알 수 있는 것들은 다음과 같습니다:

1. 주어지는 파라미터와 엔터티에 따라 달라지는 합성 데이터셋 샘플을 통해 다양성 식별하기
2. 빈칸을 채우기위한 엔터티 집합을 생성하거나 컴파일하기
3. 삽입(insertion)할 엔터티를 임의로 선택하여 데이터셋을 생성하기. generation temperature는 기본값보다 높지만 최대값보다 낮게 설정하는 것이 가장 좋습니다
4. ChatGPT/GPT-4의 생성 결과를 통해 로컬 모델을 학습하기

엔터티 중 하나가 generation 클래스 레이블이 될 수 있음을 꼭 기억하세요. 예를 들어,  예를 들어, 감정 분류(classification) 작업에서 "긍정적" 또는 "부정적"이라는 단어를 프롬프트에 직접적으로 언급하여 해당 레이블과 함께 생성된 텍스트를 추출할 수 있습니다. 그런 다음 이 데이터를 기반으로 로컬 분류기(classifier)는 학습할 수 있겠지요.

# 반복적(계층적) 합성 데이터셋 생성

포맷은 더 많은 엔터티를 사용하여 더욱 복잡하게 구성할 수 있습니다, 몇몇은 LLM 그 자체 이전에 생성되기도 했고요. 예를 들어, 이야기 요약과 함께 특정 문장을 꼭 포함해달라는 요청(request)으로 시작해보겠습니다. 추후, 생성된 중간 데이터를 최종 요청에 사용하기만 하면 됩니다.

*프롬프트:*
```
요약: {위의 접근법을 사용한 LLM으로 생성된 짧은 요약}
특징: {최초(initial) 프롬프트의 특징을 복사할 것}
문장: {이야기에 등장한 LLM으로 생성된 문장}
단어: {최초(initial) 프롬프트의 단어를 복사할 것}
이야기:
```

빈칸이 채워진 프롬프트 예시:

*프롬프트:*
```
요약: Lily와 Timmy는 함께 모래성을 쌓으며 협동하는 방법을 배웁니다. 하지만 돌풍에 의해 모래성은 무너지고 무너집니다. 그 둘은 무너진 모래성에서 아름다움을 찾아내고 나비와 함께 즐겁게 놀아요. 
특징: 대화, 징조, 반전
문장: 어느날, 그녀는 공원에서 아름다운 나비를 보았습니다.
단어: 대립하다, 네트워크, 아름다운
Story:
```

*결과물:*
```
옛날 옛적에, Lily라는 소녀가 살았어요. 그녀는 친구와 함께 장난감 놀이를 즐겼습니다. 어느날, 그녀는 공원에서 아름다운 나비를 보았습니다. 그녀는 나비를 잡으려했으나 날아가버렸어요. Lily의 친구인 Timmy는 그녀와 놀러 나왔어요. 그 둘은 모래성을 쌓기로 했지만 서로 모래성을 어떻게 쌓을지에 대해서 대립했습니다. Lily는 높은 모래성을 원했지만 Timmy는 넓게 만들고 싶어했어요. 잠시 언쟁이 있은 후, 마침내 높고 넓은 모래성을 만들기로 합의를 봤습니다. 모래성을 짓는 도중, 개미 떼가 일하는 것을 목격했어요. 개미떼가 터널 네트워크를 짓고있다는 것을 발견했지요. 갑자기, 돌풍이 불어와 모래성을 무너뜨렸어요. 그들은 슬펐지만 이내 나비를 다시 보게되었습니다. 나비가 무너진 모래성에 앉았고 그것을 보며 무너진 모래성 그 자체도 아름다울 수 있음을 깨달았어요. 그들은 다시 웃으며 놀이를 계속했습니다.
```

이처럼 모델을 학습시키기 위해 수십만 개의 매우 다양한 예제를 생성할 수 있습니다. 분류기(classifier)로 하여금 텍스트에 대화나 반전(plot twist)이 있는지 알아낼 수 있도록 훈련을 해야한다고 가정해봅시다. 최초(initial) 프롬프트가 레이블을 가지고 있으므로, 생성된 각 샘플에 대해 어떤 목표값(target value)을 예측해야 하는지 알려져 있습니다.

# 당신에게 필요한 그것은 바로 교과서(Textbooks)

이 접근 방식에서 기인하는 중요한 질문은 데이터셋 합성이 실제 애플리케이션을 위한 네트워크를 훈련할 때 진정으로 이점을 제공할 수 있는지 여부입니다. 다행히 저자들은 조사를 수행할 때 최첨단 LLM에서 파생된 합성 데이터를 사용하여 더 작은 언어 모델을 훈련하는 효과를 검증함으로써 이 문제를 해결했습니다.

그들의 연구인 [Gunasekar et al. (2023)] (https://arxiv.org/abs/2306.11644)에서는 모델에 제공되는 훈련 데이터의 품질을 강조합니다. 흔히 우리가 "교과서(textbook)"라고 부르는 명확하고 편견이 없고 포괄적이며 다양한 정보를 기반으로 언어 모델 트레이닝을 실행하면 더 효과적일 것이라고 연구진은 주장합니다.

이러한 원리를 기반으로 Phi-1이라는 이름을 가진 LLM을 훈련시키기 위한 준합성(semi-synthetic) 데이터셋을 만드는 기초를 형성했습니다. 주요 평가과제는 주어진 텍스트 설명이나 독스트링(docstring)을 준수하는 Python 함수를 생성하는 것입니다. 모델의 품질은 HumanEval benchmark([Chen et al., 2021](https://arxiv.org/abs/2107.03374))로 평가합니다.

이 접근법에서 저자는 여러 이유로 다양성의 중요성을 강조합니다. 언어 모델을 다양한 코드 표현식과 문제 해결 접근 방식에 노출시키고, 과적합(overfitting)하거나 특정 패턴에 의존할 위험을 줄이며, 익숙하지 않거나 혁신적 작업을 처리하는 모델의 성능을 향상시킵니다.

코드작성 문제를 해결하기 위해 저자들은 추론(reasoning)이나 기본적인 알고리즘 기술에  초점을 맞춘 교과서 같은 문서를 만들었습니다. 다양성을 달성하기위해 다음과 같은 제한을 두었습니다:

- 주제
- 대상 이용자(target audience)

아쉽게도, 저자들은 합성 데이터를 생성하는 데 사용한 프롬프트 템플릿에 대한 구체적인 정보를 제공하지 않았습니다. 하지만 그들은 결과물을 설명해주었습니다. 그들은 GPT-4 대신 ChatGPT (GPT-3.5)를 사용하기로 결정했고, 이 전략 또한 합성 데이터를 사용하여 모델을 파인튜닝 했기 때문에 우수한 결과를 얻었습니다.

<Screenshot src={IMG1} alt="생성된 교과서 같은 샘플" />

이미지 출처: [Gunasekar et al. (2023)](https://arxiv.org/abs/2306.11644)

대상 이용자(target audience)를 염두에 두는 것은 중요합니다. 대상 이용자를 지정하는 것은 합성 데이터 다양성을 향상시키는 훌륭한 방법입니다. 고등학생, 컴공 학부생, 그리고 박사 과정 지원자는 분명히 설명과 맥락(process)을 저마다 다르게 표현하므로 생성된 데이터는 더욱 다양할 것입니다. 다시 프롬프트를 작성해볼까요.


*프롬프트:*
```
학사 1학년 수준의 컴퓨터 과학 교과서의 발췌문을 작성해줘. Python 3.6으로 코드를 작성해줘.

다음에서 언급할 주제의 내용을 발췌(extract)해줘: 단수 행렬(Singular matrices).

주어진 주제의 높은 수준부터 발췌를 해줘. 그런 다음 예제와 그에 따른 해결책을 자연어로 설명해줘. 그 후 예제에 이어 1-2개의 코드 스니펫을 함께 제공해줘. 각 스니펫은 10줄 이하로 작성해줘. 코드 스니펫 뒤에는 텍스트가 없도록 해줘

발췌문은 간결하게 한 번에 하나의 개념만 설명해야 한다는 것을 명심해줘. 코드에 Python 함수와 그 호출을 명시해줘. 모든 ML/DS 라이브러리를 사용해도 좋아.

너무 장황하게 설명하지 말아줘. 발췌문은 최대 2문단을 넘지 말아줘.
```

*결과물:*
```
**단수 행렬**

선형 대수학에서 "단수"는 행렬이 역행렬을 갖지 않는 경우를 뜻합니다. 또 다른 동등한 정의는 행렬의 식이 0일 때입니다. 단수 행렬은 유일한 해가 없는 일차 방정식 시스템을 나타내기 때문에 중요합니다.

2x2 행렬 `[[a, b], [c, d]]`를 떠올려보세요. 만약 `ad`와 `bc`의 곱이 같다면(즉, `ad = bc`), 이 행렬은 단수입니다. 다음 예시를 살펴봅시다: 행렬 `[[2, 4], [1, 2]]`가 주어지면 대각선의 곱은 동일한 결과를 얻으므로 (2 * 2 = 4 * 1), 이것은 단수 행렬입니다.

import numpy as np

def is_singular(matrix):
    determinant = np.linalg.det(matrix)
    return abs(determinant) < 1e-9

matrix_example = np.array([[2, 4], [1, 2]])
print(is_singular(matrix_example))  # True를 반환
```

거의 다 왔습니다!

결과적으로, 저자는 모델의 훈련 세트를 보강하기 위해 총 1B 토큰을 생성하여 더 작은 모델(오직 1.5B 매개변수를 취하는)이 크기의 10배에 달하는 경쟁 모델에 도달할 수 있도록 했습니다.(자세한 내용은 기사 [Gunasekar et al. (2023)](https://arxiv.org/abs/2306.11644) 를 참조하세요.) 

<Screenshot src={IMG2} alt="Phi-1 metrics, compared to bigger models." />

이미지 출처: [Gunasekar et al. (2023)](https://arxiv.org/abs/2306.11644)

이 글을 읽는 여러분들은 아마 위에 언급된 저자들 만큼 많은 양의 합성 데이터가 필요하지는 않을 것입니다. 그들은 상당한 리소스를 필요로하는 사전 연구를 해왔으니까요. 하지만 어림잡아도, 1,000개의 토큰 당 `$0.002`의 가격(ChatGPT 표준 가격)으로, 지금까지 생성된 토큰만해도`$2000`상당이며 같은 양의 프롬프트의 경우에도 거의 같은 금액이 들 것입니다.

특히 사용 언어가 영어가 아닌 경우, 도메인이 비주류일수록 합성 데이터에 실시하는 파인튜닝의 가치는 더 높아진다는 것을 명심해야 합니다. 또한 이 방법은 [생각의 사슬(CoT)](https://www.promptingguide.ai/techniques/cot)과도 잘 작동하여 로컬 모델의 추론 능력(reasoning capabilities)을 향상시킵니다. 다른 프롬프트 기법도 작동합니다. 그리고 Alpaca([Taori et al., (2023)](https://crfm.stanford.edu/2023/03/13/alpaca.html)) 와 Vicuna([Zheng et al., (2023)](https://lmsys.org/blog/2023-03-30-vicuna/)) 와 같은 오픈 소스 모델은 합성 데이터에 대한 파인튜닝을 통해 기능을 더 발전시킵니다.
