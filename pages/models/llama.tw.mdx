## LLaMA：開放而且高效能的基礎語言模型

<Callout emoji="⚠️">
  本章節內容正在積極準備中。
</Callout>


import {Screenshot} from 'components/screenshot'
import { Callout, FileTree } from 'nextra-theme-docs'
import LLAMA1 from '../../img/llama-1.png'

## 有什麼新訊息？

這篇論文介紹了一系列的基礎語言模型，參數從 70 億到 650 億。

這些模型是在公開取得的資料集上，經過數兆個 token 的訓練。

[(Hoffman et al. 2022)](https://arxiv.org/abs/2203.15556) 的研究顯示，在有限的計算資源下，使用更多資料來訓練的小型模型可以達到比大型模型更好的效能。該研究建議用 2000 億個 token 來訓練 100 億的模型。然而，LLaMA 論文發現，即使在 1 兆個 token 之後，70 億模型的效能仍然有提升。

<Screenshot src={LLAMA1} alt="LLAMA1" />

這項研究專注於訓練模型（LLaMA）以在不同的推論預算下實現最佳效能，透過使用更多的 token。


## 功能與主要成果

整體而言，LLaMA-13B 在多數效能測試上超越了 GPT-3（175 億），儘管它比 GPT-3 小了 10 倍，並且可以在單一的 GPU 上執行。LLaMA 65B 與 Chinchilla-70B 和 PaLM-540B 等模型具有競爭力。

*論文：* [LLaMA：開放而且高效能的基礎語言模型](https://arxiv.org/abs/2302.13971)

*程式碼：* https://github.com/facebookresearch/llama

## 參考文獻

- [Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/) (April 2023)
- [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://arxiv.org/abs/2304.01196) (April 2023)
- [Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality](https://vicuna.lmsys.org/) (March 2023)
- [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199) (March 2023)
- [GPT4All](https://github.com/nomic-ai/gpt4all) (March 2023)
- [ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge](https://arxiv.org/abs/2303.14070) (March 2023)
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) (March 2023)
