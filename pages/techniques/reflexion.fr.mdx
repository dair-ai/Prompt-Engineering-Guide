# Reflexion

import { Callout } from 'nextra/components'

Reflexion est un cadre de travail pour renforcer les agents bas√©s sur le langage gr√¢ce au feedback linguistique. Selon [Shinn et al. (2023)](https://arxiv.org/pdf/2303.11366.pdf), "Reflexion est un nouveau paradigme de renforcement ‚Äòverbal‚Äò qui param√®tre une politique en tant qu'encodage de la m√©moire d'un agent, associ√© √† un choix de param√®tres du LLM."

√Ä un haut niveau, Reflexion convertit les feedbacks de l'environnement (qu'ils soient sous forme de texte libre ou de valeurs scalaires) en feedback linguistique, √©galement appel√© **autor√©flexion**. Ce feedback est fourni comme contexte √† un agent LLM pour l'√©pisode suivant. Cela aide l'agent √† apprendre rapidement et efficacement de ses erreurs pass√©es, ce qui entra√Æne des am√©liorations de ses performances sur de nombreuses t√¢ches avanc√©es.

!["M√©thodologie Reflexion"](../../img/research/reflexion.png)

Comme le montre la figure ci-dessus, Reflexion se compose de trois mod√®les distincts:

- **Un Acteur**: Il g√©n√®re du texte et des actions bas√©es sur les observations de l'√©tat. L'Acteur effectue une action dans un environnement et re√ßoit une observation, ce qui cr√©e une trajectoire. Le [Chain-of-Thought (CoT)](https://www.promptingguide.ai/fr/techniques/cot) et [ReAct](https://www.promptingguide.ai/fr/techniques/react) sont utilis√©s comme mod√®les d'Acteur. Un composant de m√©moire est √©galement ajout√© pour fournir un contexte suppl√©mentaire √† l'agent.
- **Un √âvaluateur**: Il attribue un score aux sorties produites par l'Acteur. Concr√®tement, il prend en entr√©e une trajectoire g√©n√©r√©e (√©galement appel√©e m√©moire √† court terme) et donne en sortie un score de r√©compense. Diff√©rentes fonctions de r√©compense sont utilis√©es en fonction de la t√¢che (les LLM et les heuristiques bas√©es sur des r√®gles sont utilis√©es pour les t√¢ches de prise de d√©cision).
- **L'Auto-r√©flexion**: Elle g√©n√®re des signaux de renforcement verbaux pour aider l'Acteur √† s'am√©liorer. Ce r√¥le est assur√© par un LLM qui fournit un feedback pr√©cieux pour les essais futurs. Pour g√©n√©rer un feedback sp√©cifique et pertinent, qui est √©galement stock√© en m√©moire, le mod√®le d'auto-r√©flexion utilise le signal de r√©compense, la trajectoire actuelle et sa m√©moire persistante. Ces exp√©riences (stock√©es dans la m√©moire √† long terme) sont exploit√©es par l'agent pour am√©liorer rapidement sa prise de d√©cision.

En r√©sum√©, les √©tapes cl√©s du processus Reflexion sont : a) d√©finir une t√¢che, b) g√©n√©rer une trajectoire, c) √©valuer, d) effectuer une r√©flexion et e) g√©n√©rer la trajectoire suivante. La figure ci-dessous montre des exemples de la mani√®re dont un agent Reflexion peut apprendre √† optimiser de mani√®re it√©rative son comportement pour r√©soudre diverses t√¢ches comme la prise de d√©cision, la programmation et le raisonnement. Reflexion √©tend le cadre de travail ReAct en y introduisant l'auto-√©valuation, l'auto-r√©flexion et des composants de m√©moire.

!["Exemples de Reflexion"](../../img/research/reflexion-examples.png)

En r√©sum√©, les √©tapes cl√©s du processus Reflexion sont : a) d√©finir une t√¢che, b) g√©n√©rer une trajectoire, c) √©valuer, d) effectuer une r√©flexion et e) g√©n√©rer la trajectoire suivante. La figure ci-dessous montre des exemples de la mani√®re dont un agent Reflexion peut apprendre √† optimiser de mani√®re it√©rative son comportement pour r√©soudre diverses t√¢ches comme la prise de d√©cision, la programmation et le raisonnement. Reflexion √©tend le cadre de travail ReAct en y introduisant l'auto-√©valuation, l'auto-r√©flexion et des composants de m√©moire.

## R√©sultats

Les r√©sultats exp√©rimentaux montrent que les agents Reflexion am√©liorent de mani√®re significative leurs performances sur les t√¢ches de prise de d√©cision AlfWorld, les questions de raisonnement de HotPotQA et les t√¢ches de programmation Python sur HumanEval.

√âvalu√© sur les t√¢ches de prise de d√©cision s√©quentielle (AlfWorld), ReAct + Reflexion surpasse nettement ReAct en compl√©tant 130 t√¢ches sur 134 en utilisant des techniques d'auto-√©valuation heuristiques et GPT pour la classification binaire.

!["R√©sultats de Reflexion sur ALFWorld"](../../img/research/reflexion-alfworld.png)

Reflexion surpasse de mani√®re significative toutes les approches de r√©f√©rence sur plusieurs √©tapes d'apprentissage. Pour le raisonnement seul et lorsque l'on ajoute une m√©moire √©pisodique compos√©e de la trajectoire la plus r√©cente, Reflexion + CoT surpasse respectivement le CoT seul et le CoT avec m√©moire √©pisodique.

!["R√©sultats de Reflexion sur ALFWorld"](../../img/research/reflexion-hotpotqa.png)

Comme le r√©sume le tableau ci-dessous, Reflexion surpasse g√©n√©ralement les approches de pointe pr√©c√©dentes pour l'√©criture de code en Python et en Rust sur MBPP, HumanEval et Leetcode Hard.

!["Reflexion ALFWorld Results"](../../img/research/reflexion-programming.png)


## Quand utiliser Reflexion ?

Reflexion est particuli√®rement adapt√© aux situations suivantes:

1. **Un agent doit apprendre par essai et erreur**: Reflexion est con√ßu pour aider les agents √† am√©liorer leurs performances en r√©fl√©chissant sur leurs erreurs pass√©es et en int√©grant ces connaissances dans leurs futures d√©cisions. Cela le rend bien adapt√© aux t√¢ches o√π l'agent doit apprendre par essai et erreur, comme la prise de d√©cision, le raisonnement et la programmation.

2. **Les m√©thodes d'apprentissage par renforcement traditionnelles sont peu pratiques**: Les m√©thodes d'apprentissage par renforcement (RL) traditionnelles n√©cessitent souvent de grandes quantit√©s de donn√©es d'entra√Ænement et un r√©glage fin (fine-tuning) co√ªteux des mod√®les. Reflexion offre une alternative l√©g√®re qui ne n√©cessite pas de fine-tuning du mod√®le de langage sous-jacent, ce qui le rend plus efficace en termes de donn√©es et de ressources de calcul.

3. **Un feedback nuanc√© est n√©cessaire**: Reflexion utilise un feedback verbal, qui peut √™tre plus nuanc√© et sp√©cifique que les r√©compenses scalaires utilis√©es dans le RL traditionnel. Cela permet √† l'agent de mieux comprendre ses erreurs et d'apporter des am√©liorations plus cibl√©es lors des essais suivants.

4. **L'interpr√©tabilit√© et une m√©moire explicite sont importantes**: Reflexion offre une forme de m√©moire √©pisodique plus interpr√©table et explicite que les m√©thodes de RL traditionnelles. Les auto-r√©flexions de l'agent sont stock√©es dans sa m√©moire, ce qui facilite l'analyse et la compr√©hension de son processus d'apprentissage.

Reflexion est efficace dans les t√¢ches suivantes:

- **Prise de d√©cision s√©quentielle**: Les agents Reflexion am√©liorent leurs performances sur les t√¢ches AlfWorld, qui impliquent de naviguer dans divers environnements et de mener √† bien des objectifs en plusieurs √©tapes.
- **Raisonnement**: Reflexion a am√©lior√© la performance des agents sur HotPotQA, un ensemble de donn√©es de questions-r√©ponses qui n√©cessite un raisonnement √† travers plusieurs documents.
- **Programmation**: Les agents Reflexion √©crivent un meilleur code sur des benchmarks comme HumanEval et MBPP, atteignant des r√©sultats de pointe dans certains cas.

Voici quelques limitations de Reflexion:

- **D√©pendance √† la capacit√© d'auto-√©valuation**: Reflexion repose sur la capacit√© de l'agent √† √©valuer avec pr√©cision sa performance et √† g√©n√©rer des auto-r√©flexions utiles. Cela peut √™tre un d√©fi, surtout pour des t√¢ches complexes, mais il est pr√©vu que Reflexion s'am√©liore avec le temps √† mesure que les capacit√©s des mod√®les continuent de progresser.
- **Contraintes de m√©moire √† long terme**: Reflexion utilise une fen√™tre glissante avec une capacit√© maximale, mais pour des t√¢ches plus complexes, il pourrait √™tre avantageux d'utiliser des structures avanc√©es telles que l'int√©gration vectorielle (vector embedding) ou les bases de donn√©es SQL.
- **Limitations de la g√©n√©ration de code**: Le d√©veloppement pilot√© par les tests (test-driven development) pr√©sente des limites dans la sp√©cification pr√©cise des correspondances entr√©e-sortie (par exemple, une fonction g√©n√©ratrice non d√©terministe et des sorties de fonction influenc√©es par le mat√©riel).

---

*Source des figures: [Reflexion : Agents linguistiques avec apprentissage par renforcement verbal](https://arxiv.org/pdf/2303.11366.pdf)*

<Callout type= "info" emoji="üéì">
Apprenez-en davantage sur les m√©thodes d'ing√©nierie des prompts avanc√©es dans nos nouveaux cours sur l'IA. [Join now!](https://dair-ai.thinkific.com/).
Utilisez le code PROMPTING20 pour obtenir une r√©duction suppl√©mentaire de 20 %.
</Callout>

## References

- [Reflexion : Agents linguistiques avec apprentissage par renforcement verbal](https://arxiv.org/pdf/2303.11366.pdf)
- [Les LLMs peuvent-ils critiquer et am√©liorer leurs propres sorties?](https://evjang.com/2023/03/26/self-reflection.html)
